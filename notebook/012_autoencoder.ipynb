{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('max_columns', 300)\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils import create_save_folder, EarlyStopping\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "fm_path = create_save_folder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class denoising_model(nn.Module):\n",
    "    def __init__(self, num_columns):\n",
    "        super(denoising_model,self).__init__()\n",
    "        self.encoder=nn.Sequential(\n",
    "            nn.Linear(num_columns,256),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            nn.SiLU(True),\n",
    "            nn.Linear(256,128),\n",
    "            # nn.BatchNorm1d(128),\n",
    "            nn.SiLU(True),\n",
    "        )\n",
    "        \n",
    "        self.decoder=nn.Sequential(\n",
    "            nn.Linear(128,256),\n",
    "            # nn.BatchNorm1d(256),\n",
    "            nn.SiLU(True),\n",
    "            nn.Linear(256, num_columns),\n",
    "            # nn.BatchNorm1d(num_columns),\n",
    "            nn.SiLU(True),\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet:\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # noise = torch.randn(self.data.shape[1]).cuda()\n",
    "        # clean = self.data[index]\n",
    "        # dirty = self.data[index] + noise\n",
    "        # return clean, dirty\n",
    "        return self.data[index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/yoshikawa/work/kaggle/OPVP/output/feature_model/20210824/0/train.pkl', 'rb') as f:\n",
    "    df_train = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.drop(['row_id', 'target'], axis=1)\n",
    "for col in train.columns.to_list():\n",
    "    train[col] = train[col].fillna(train[col].mean())\n",
    "\n",
    "scales = train.drop([\"stock_id\"], axis = 1).columns.to_list()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train[scales])\n",
    "train[scales] = scaler.transform(train[scales])\n",
    "le = LabelEncoder()\n",
    "le.fit(train[\"stock_id\"])\n",
    "train[\"stock_id\"] = le.transform(train[\"stock_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = torch.tensor(train.values.astype(np.float32)).cuda()\n",
    "train_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  0\n",
      "====================================================================================================\n",
      "Validation loss decreased (inf --> 0.426466).  Saving model ...\n",
      "Validation loss decreased (0.426466 --> 0.341160).  Saving model ...\n",
      "Validation loss decreased (0.341160 --> 0.304301).  Saving model ...\n",
      "Validation loss decreased (0.304301 --> 0.287294).  Saving model ...\n",
      "Validation loss decreased (0.287294 --> 0.279119).  Saving model ...\n",
      "Validation loss decreased (0.279119 --> 0.274693).  Saving model ...\n",
      "Validation loss decreased (0.274693 --> 0.269382).  Saving model ...\n",
      "Validation loss decreased (0.269382 --> 0.261899).  Saving model ...\n",
      "Validation loss decreased (0.261899 --> 0.259224).  Saving model ...\n",
      "10  epoch - train_loss:  0.2604 , val_loss:  0.2558\n",
      "Validation loss decreased (0.259224 --> 0.255815).  Saving model ...\n",
      "Validation loss decreased (0.255815 --> 0.254173).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.254173 --> 0.249711).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.249711 --> 0.246862).  Saving model ...\n",
      "Validation loss decreased (0.246862 --> 0.246067).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.246067 --> 0.243300).  Saving model ...\n",
      "Validation loss decreased (0.243300 --> 0.242033).  Saving model ...\n",
      "20  epoch - train_loss:  0.2456 , val_loss:  0.2415\n",
      "Validation loss decreased (0.242033 --> 0.241498).  Saving model ...\n",
      "Validation loss decreased (0.241498 --> 0.241334).  Saving model ...\n",
      "Validation loss decreased (0.241334 --> 0.240445).  Saving model ...\n",
      "Validation loss decreased (0.240445 --> 0.238936).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.238936 --> 0.236604).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.236604 --> 0.235123).  Saving model ...\n",
      "30  epoch - train_loss:  0.2377 , val_loss:  0.2352\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.235123 --> 0.233955).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.233955 --> 0.233727).  Saving model ...\n",
      "Validation loss decreased (0.233727 --> 0.233024).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.233024 --> 0.232487).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.232487 --> 0.232158).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "40  epoch - train_loss:  0.2341 , val_loss:  0.2311\n",
      "Validation loss decreased (0.232158 --> 0.231092).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.231092 --> 0.231042).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.231042 --> 0.230098).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.230098 --> 0.229697).  Saving model ...\n",
      "Validation loss decreased (0.229697 --> 0.229441).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "50  epoch - train_loss:  0.2319 , val_loss:  0.2289\n",
      "Validation loss decreased (0.229441 --> 0.228874).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.228874 --> 0.227975).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.227975 --> 0.227154).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.227154 --> 0.227041).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "60  epoch - train_loss:  0.2299 , val_loss:  0.2265\n",
      "Validation loss decreased (0.227041 --> 0.226545).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.226545 --> 0.226312).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.226312 --> 0.225763).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.225763 --> 0.225597).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "70  epoch - train_loss:  0.2282 , val_loss:  0.225\n",
      "Validation loss decreased (0.225597 --> 0.225008).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.225008 --> 0.224861).  Saving model ...\n",
      "Validation loss decreased (0.224861 --> 0.224036).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "80  epoch - train_loss:  0.2269 , val_loss:  0.2244\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.224036 --> 0.224017).  Saving model ...\n",
      "Validation loss decreased (0.224017 --> 0.223609).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.223609 --> 0.223536).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.223536 --> 0.222933).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "90  epoch - train_loss:  0.2259 , val_loss:  0.2245\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.222933 --> 0.222746).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.222746 --> 0.222370).  Saving model ...\n",
      "Validation loss decreased (0.222370 --> 0.222261).  Saving model ...\n",
      "Validation loss decreased (0.222261 --> 0.221986).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.221986 --> 0.221972).  Saving model ...\n",
      "100  epoch - train_loss:  0.2254 , val_loss:  0.2224\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.221972 --> 0.221776).  Saving model ...\n",
      "Validation loss decreased (0.221776 --> 0.221574).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.221574 --> 0.221521).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.221521 --> 0.221218).  Saving model ...\n",
      "110  epoch - train_loss:  0.224 , val_loss:  0.2216\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.221218 --> 0.220665).  Saving model ...\n",
      "Validation loss decreased (0.220665 --> 0.220620).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.220620 --> 0.220465).  Saving model ...\n",
      "Validation loss decreased (0.220465 --> 0.220330).  Saving model ...\n",
      "120  epoch - train_loss:  0.2235 , val_loss:  0.2207\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.220330 --> 0.220329).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.220329 --> 0.220064).  Saving model ...\n",
      "Validation loss decreased (0.220064 --> 0.219906).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "130  epoch - train_loss:  0.2225 , val_loss:  0.2199\n",
      "Validation loss decreased (0.219906 --> 0.219904).  Saving model ...\n",
      "Validation loss decreased (0.219904 --> 0.219787).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.219787 --> 0.219698).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.219698 --> 0.219174).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "140  epoch - train_loss:  0.2217 , val_loss:  0.2206\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.219174 --> 0.218879).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "150  epoch - train_loss:  0.2213 , val_loss:  0.2187\n",
      "Validation loss decreased (0.218879 --> 0.218650).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.218650 --> 0.218347).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.218347 --> 0.218105).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "160  epoch - train_loss:  0.221 , val_loss:  0.2185\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.218105 --> 0.218075).  Saving model ...\n",
      "Validation loss decreased (0.218075 --> 0.218011).  Saving model ...\n",
      "Validation loss decreased (0.218011 --> 0.217786).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.217786 --> 0.217752).  Saving model ...\n",
      "Validation loss decreased (0.217752 --> 0.217633).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "170  epoch - train_loss:  0.2202 , val_loss:  0.218\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.217633 --> 0.217468).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.217468 --> 0.217209).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.217209 --> 0.217120).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "180  epoch - train_loss:  0.2202 , val_loss:  0.2173\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.217120 --> 0.216992).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "190  epoch - train_loss:  0.2199 , val_loss:  0.2172\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Validation loss decreased (0.216992 --> 0.216868).  Saving model ...\n",
      "Validation loss decreased (0.216868 --> 0.216823).  Saving model ...\n",
      "Validation loss decreased (0.216823 --> 0.216517).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.216517 --> 0.216324).  Saving model ...\n",
      "200  epoch - train_loss:  0.2193 , val_loss:  0.2172\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.216324 --> 0.216022).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "210  epoch - train_loss:  0.2189 , val_loss:  0.2163\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early Stopping!!\n",
      "fold:  1\n",
      "====================================================================================================\n",
      "Validation loss decreased (inf --> 0.437864).  Saving model ...\n",
      "Validation loss decreased (0.437864 --> 0.355830).  Saving model ...\n",
      "Validation loss decreased (0.355830 --> 0.316593).  Saving model ...\n",
      "Validation loss decreased (0.316593 --> 0.300181).  Saving model ...\n",
      "Validation loss decreased (0.300181 --> 0.282890).  Saving model ...\n",
      "Validation loss decreased (0.282890 --> 0.274077).  Saving model ...\n",
      "Validation loss decreased (0.274077 --> 0.268765).  Saving model ...\n",
      "Validation loss decreased (0.268765 --> 0.268009).  Saving model ...\n",
      "Validation loss decreased (0.268009 --> 0.267969).  Saving model ...\n",
      "10  epoch - train_loss:  0.2569 , val_loss:  0.2565\n",
      "Validation loss decreased (0.267969 --> 0.256455).  Saving model ...\n",
      "Validation loss decreased (0.256455 --> 0.253870).  Saving model ...\n",
      "Validation loss decreased (0.253870 --> 0.252594).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.252594 --> 0.248402).  Saving model ...\n",
      "Validation loss decreased (0.248402 --> 0.246712).  Saving model ...\n",
      "Validation loss decreased (0.246712 --> 0.246482).  Saving model ...\n",
      "Validation loss decreased (0.246482 --> 0.244567).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.244567 --> 0.243000).  Saving model ...\n",
      "20  epoch - train_loss:  0.2406 , val_loss:  0.2409\n",
      "Validation loss decreased (0.243000 --> 0.240949).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.240949 --> 0.240759).  Saving model ...\n",
      "Validation loss decreased (0.240759 --> 0.239853).  Saving model ...\n",
      "Validation loss decreased (0.239853 --> 0.238903).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.238903 --> 0.236928).  Saving model ...\n",
      "Validation loss decreased (0.236928 --> 0.236792).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.236792 --> 0.235959).  Saving model ...\n",
      "30  epoch - train_loss:  0.2343 , val_loss:  0.2352\n",
      "Validation loss decreased (0.235959 --> 0.235185).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.235185 --> 0.235037).  Saving model ...\n",
      "Validation loss decreased (0.235037 --> 0.233595).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.233595 --> 0.233353).  Saving model ...\n",
      "Validation loss decreased (0.233353 --> 0.233009).  Saving model ...\n",
      "Validation loss decreased (0.233009 --> 0.231954).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "40  epoch - train_loss:  0.2302 , val_loss:  0.2335\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.231954 --> 0.231232).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.231232 --> 0.231021).  Saving model ...\n",
      "Validation loss decreased (0.231021 --> 0.229752).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.229752 --> 0.228794).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.228794 --> 0.228723).  Saving model ...\n",
      "50  epoch - train_loss:  0.228 , val_loss:  0.2286\n",
      "Validation loss decreased (0.228723 --> 0.228640).  Saving model ...\n",
      "Validation loss decreased (0.228640 --> 0.228619).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.228619 --> 0.228440).  Saving model ...\n",
      "Validation loss decreased (0.228440 --> 0.228006).  Saving model ...\n",
      "Validation loss decreased (0.228006 --> 0.227999).  Saving model ...\n",
      "Validation loss decreased (0.227999 --> 0.227356).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.227356 --> 0.226838).  Saving model ...\n",
      "60  epoch - train_loss:  0.2255 , val_loss:  0.2274\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.226838 --> 0.226748).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.226748 --> 0.225963).  Saving model ...\n",
      "Validation loss decreased (0.225963 --> 0.225886).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.225886 --> 0.225498).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.225498 --> 0.224916).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "70  epoch - train_loss:  0.2242 , val_loss:  0.2251\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Validation loss decreased (0.224916 --> 0.224340).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.224340 --> 0.224156).  Saving model ...\n",
      "80  epoch - train_loss:  0.2231 , val_loss:  0.2242\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.224156 --> 0.223707).  Saving model ...\n",
      "Validation loss decreased (0.223707 --> 0.223653).  Saving model ...\n",
      "Validation loss decreased (0.223653 --> 0.223113).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "90  epoch - train_loss:  0.2219 , val_loss:  0.2228\n",
      "Validation loss decreased (0.223113 --> 0.222768).  Saving model ...\n",
      "Validation loss decreased (0.222768 --> 0.222721).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.222721 --> 0.222116).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.222116 --> 0.221975).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.221975 --> 0.221904).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "100  epoch - train_loss:  0.221 , val_loss:  0.222\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.221904 --> 0.221301).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.221301 --> 0.221176).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.221176 --> 0.220878).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "110  epoch - train_loss:  0.2203 , val_loss:  0.2212\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.220878 --> 0.220649).  Saving model ...\n",
      "Validation loss decreased (0.220649 --> 0.220445).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.220445 --> 0.220139).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "120  epoch - train_loss:  0.2188 , val_loss:  0.2198\n",
      "Validation loss decreased (0.220139 --> 0.219849).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.219849 --> 0.219719).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.219719 --> 0.219559).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.219559 --> 0.219317).  Saving model ...\n",
      "Validation loss decreased (0.219317 --> 0.219177).  Saving model ...\n",
      "Validation loss decreased (0.219177 --> 0.219136).  Saving model ...\n",
      "130  epoch - train_loss:  0.2181 , val_loss:  0.2191\n",
      "Validation loss decreased (0.219136 --> 0.219119).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.219119 --> 0.218561).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.218561 --> 0.218354).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.218354 --> 0.218000).  Saving model ...\n",
      "140  epoch - train_loss:  0.2175 , val_loss:  0.219\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early Stopping!!\n",
      "fold:  2\n",
      "====================================================================================================\n",
      "Validation loss decreased (inf --> 0.414439).  Saving model ...\n",
      "Validation loss decreased (0.414439 --> 0.336328).  Saving model ...\n",
      "Validation loss decreased (0.336328 --> 0.297984).  Saving model ...\n",
      "Validation loss decreased (0.297984 --> 0.285185).  Saving model ...\n",
      "Validation loss decreased (0.285185 --> 0.272489).  Saving model ...\n",
      "Validation loss decreased (0.272489 --> 0.267361).  Saving model ...\n",
      "Validation loss decreased (0.267361 --> 0.261156).  Saving model ...\n",
      "Validation loss decreased (0.261156 --> 0.255082).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "10  epoch - train_loss:  0.2485 , val_loss:  0.2497\n",
      "Validation loss decreased (0.255082 --> 0.249691).  Saving model ...\n",
      "Validation loss decreased (0.249691 --> 0.247111).  Saving model ...\n",
      "Validation loss decreased (0.247111 --> 0.246430).  Saving model ...\n",
      "Validation loss decreased (0.246430 --> 0.243193).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.243193 --> 0.239592).  Saving model ...\n",
      "Validation loss decreased (0.239592 --> 0.238269).  Saving model ...\n",
      "Validation loss decreased (0.238269 --> 0.237175).  Saving model ...\n",
      "Validation loss decreased (0.237175 --> 0.236956).  Saving model ...\n",
      "Validation loss decreased (0.236956 --> 0.236604).  Saving model ...\n",
      "20  epoch - train_loss:  0.2323 , val_loss:  0.2348\n",
      "Validation loss decreased (0.236604 --> 0.234827).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.234827 --> 0.231895).  Saving model ...\n",
      "Validation loss decreased (0.231895 --> 0.231454).  Saving model ...\n",
      "Validation loss decreased (0.231454 --> 0.230448).  Saving model ...\n",
      "Validation loss decreased (0.230448 --> 0.230441).  Saving model ...\n",
      "Validation loss decreased (0.230441 --> 0.229415).  Saving model ...\n",
      "Validation loss decreased (0.229415 --> 0.228830).  Saving model ...\n",
      "Validation loss decreased (0.228830 --> 0.228148).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "30  epoch - train_loss:  0.2246 , val_loss:  0.2279\n",
      "Validation loss decreased (0.228148 --> 0.227938).  Saving model ...\n",
      "Validation loss decreased (0.227938 --> 0.226759).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.226759 --> 0.225681).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.225681 --> 0.224156).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.224156 --> 0.223592).  Saving model ...\n",
      "40  epoch - train_loss:  0.2203 , val_loss:  0.2226\n",
      "Validation loss decreased (0.223592 --> 0.222603).  Saving model ...\n",
      "Validation loss decreased (0.222603 --> 0.221929).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.221929 --> 0.221553).  Saving model ...\n",
      "Validation loss decreased (0.221553 --> 0.221153).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.221153 --> 0.220340).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "50  epoch - train_loss:  0.2173 , val_loss:  0.2203\n",
      "Validation loss decreased (0.220340 --> 0.220326).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.220326 --> 0.220069).  Saving model ...\n",
      "Validation loss decreased (0.220069 --> 0.219561).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.219561 --> 0.219394).  Saving model ...\n",
      "Validation loss decreased (0.219394 --> 0.218622).  Saving model ...\n",
      "Validation loss decreased (0.218622 --> 0.218448).  Saving model ...\n",
      "Validation loss decreased (0.218448 --> 0.217714).  Saving model ...\n",
      "60  epoch - train_loss:  0.215 , val_loss:  0.2183\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.217714 --> 0.217676).  Saving model ...\n",
      "Validation loss decreased (0.217676 --> 0.217337).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.217337 --> 0.217173).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.217173 --> 0.215965).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "70  epoch - train_loss:  0.2137 , val_loss:  0.216\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.215965 --> 0.215948).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.215948 --> 0.215509).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.215509 --> 0.214471).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "80  epoch - train_loss:  0.212 , val_loss:  0.2156\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.214471 --> 0.214351).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.214351 --> 0.214285).  Saving model ...\n",
      "Validation loss decreased (0.214285 --> 0.214070).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.214070 --> 0.214034).  Saving model ...\n",
      "Validation loss decreased (0.214034 --> 0.213920).  Saving model ...\n",
      "Validation loss decreased (0.213920 --> 0.213788).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "90  epoch - train_loss:  0.2107 , val_loss:  0.2147\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.213788 --> 0.213316).  Saving model ...\n",
      "Validation loss decreased (0.213316 --> 0.213191).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.213191 --> 0.213014).  Saving model ...\n",
      "Validation loss decreased (0.213014 --> 0.212676).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "100  epoch - train_loss:  0.2099 , val_loss:  0.2131\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.212676 --> 0.212240).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.212240 --> 0.211967).  Saving model ...\n",
      "Validation loss decreased (0.211967 --> 0.211922).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "110  epoch - train_loss:  0.2089 , val_loss:  0.2118\n",
      "Validation loss decreased (0.211922 --> 0.211817).  Saving model ...\n",
      "Validation loss decreased (0.211817 --> 0.211470).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Validation loss decreased (0.211470 --> 0.211089).  Saving model ...\n",
      "Validation loss decreased (0.211089 --> 0.211030).  Saving model ...\n",
      "120  epoch - train_loss:  0.2079 , val_loss:  0.2111\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.211030 --> 0.211023).  Saving model ...\n",
      "Validation loss decreased (0.211023 --> 0.210949).  Saving model ...\n",
      "Validation loss decreased (0.210949 --> 0.210947).  Saving model ...\n",
      "Validation loss decreased (0.210947 --> 0.210366).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "130  epoch - train_loss:  0.208 , val_loss:  0.2107\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.210366 --> 0.210098).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.210098 --> 0.210080).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.210080 --> 0.209828).  Saving model ...\n",
      "140  epoch - train_loss:  0.2069 , val_loss:  0.2098\n",
      "Validation loss decreased (0.209828 --> 0.209812).  Saving model ...\n",
      "Validation loss decreased (0.209812 --> 0.209588).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.209588 --> 0.209526).  Saving model ...\n",
      "Validation loss decreased (0.209526 --> 0.209486).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "150  epoch - train_loss:  0.207 , val_loss:  0.2092\n",
      "Validation loss decreased (0.209486 --> 0.209240).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.209240 --> 0.209222).  Saving model ...\n",
      "Validation loss decreased (0.209222 --> 0.209082).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.209082 --> 0.208882).  Saving model ...\n",
      "160  epoch - train_loss:  0.2063 , val_loss:  0.209\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.208882 --> 0.208350).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.208350 --> 0.208345).  Saving model ...\n",
      "170  epoch - train_loss:  0.2058 , val_loss:  0.2089\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Validation loss decreased (0.208345 --> 0.208244).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "180  epoch - train_loss:  0.2055 , val_loss:  0.208\n",
      "Validation loss decreased (0.208244 --> 0.208021).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.208021 --> 0.208014).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.208014 --> 0.207655).  Saving model ...\n",
      "190  epoch - train_loss:  0.2048 , val_loss:  0.2078\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.207655 --> 0.207514).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.207514 --> 0.207412).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "200  epoch - train_loss:  0.2047 , val_loss:  0.2078\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.207412 --> 0.207235).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Validation loss decreased (0.207235 --> 0.207133).  Saving model ...\n",
      "210  epoch - train_loss:  0.2043 , val_loss:  0.208\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.207133 --> 0.207121).  Saving model ...\n",
      "Validation loss decreased (0.207121 --> 0.206998).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.206998 --> 0.206741).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "220  epoch - train_loss:  0.2042 , val_loss:  0.2069\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Validation loss decreased (0.206741 --> 0.206695).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.206695 --> 0.206545).  Saving model ...\n",
      "230  epoch - train_loss:  0.2039 , val_loss:  0.2068\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Validation loss decreased (0.206545 --> 0.206315).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.206315 --> 0.206274).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "240  epoch - train_loss:  0.2039 , val_loss:  0.207\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "Validation loss decreased (0.206274 --> 0.206212).  Saving model ...\n",
      "Validation loss decreased (0.206212 --> 0.205833).  Saving model ...\n",
      "250  epoch - train_loss:  0.2034 , val_loss:  0.2066\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early Stopping!!\n",
      "fold:  3\n",
      "====================================================================================================\n",
      "Validation loss decreased (inf --> 0.430441).  Saving model ...\n",
      "Validation loss decreased (0.430441 --> 0.341603).  Saving model ...\n",
      "Validation loss decreased (0.341603 --> 0.304187).  Saving model ...\n",
      "Validation loss decreased (0.304187 --> 0.289873).  Saving model ...\n",
      "Validation loss decreased (0.289873 --> 0.278296).  Saving model ...\n",
      "Validation loss decreased (0.278296 --> 0.269903).  Saving model ...\n",
      "Validation loss decreased (0.269903 --> 0.263000).  Saving model ...\n",
      "Validation loss decreased (0.263000 --> 0.260713).  Saving model ...\n",
      "Validation loss decreased (0.260713 --> 0.253655).  Saving model ...\n",
      "10  epoch - train_loss:  0.2526 , val_loss:  0.2518\n",
      "Validation loss decreased (0.253655 --> 0.251757).  Saving model ...\n",
      "Validation loss decreased (0.251757 --> 0.249387).  Saving model ...\n",
      "Validation loss decreased (0.249387 --> 0.246928).  Saving model ...\n",
      "Validation loss decreased (0.246928 --> 0.245361).  Saving model ...\n",
      "Validation loss decreased (0.245361 --> 0.243859).  Saving model ...\n",
      "Validation loss decreased (0.243859 --> 0.242799).  Saving model ...\n",
      "Validation loss decreased (0.242799 --> 0.241668).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.241668 --> 0.240245).  Saving model ...\n",
      "Validation loss decreased (0.240245 --> 0.238460).  Saving model ...\n",
      "20  epoch - train_loss:  0.2406 , val_loss:  0.2399\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.238460 --> 0.235596).  Saving model ...\n",
      "Validation loss decreased (0.235596 --> 0.235002).  Saving model ...\n",
      "Validation loss decreased (0.235002 --> 0.234674).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.234674 --> 0.233244).  Saving model ...\n",
      "Validation loss decreased (0.233244 --> 0.232954).  Saving model ...\n",
      "Validation loss decreased (0.232954 --> 0.232510).  Saving model ...\n",
      "Validation loss decreased (0.232510 --> 0.231751).  Saving model ...\n",
      "Validation loss decreased (0.231751 --> 0.231619).  Saving model ...\n",
      "30  epoch - train_loss:  0.231 , val_loss:  0.2305\n",
      "Validation loss decreased (0.231619 --> 0.230550).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.230550 --> 0.229346).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.229346 --> 0.228905).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.228905 --> 0.228828).  Saving model ...\n",
      "Validation loss decreased (0.228828 --> 0.227267).  Saving model ...\n",
      "40  epoch - train_loss:  0.2274 , val_loss:  0.2288\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.227267 --> 0.226671).  Saving model ...\n",
      "Validation loss decreased (0.226671 --> 0.226509).  Saving model ...\n",
      "Validation loss decreased (0.226509 --> 0.226460).  Saving model ...\n",
      "Validation loss decreased (0.226460 --> 0.225766).  Saving model ...\n",
      "Validation loss decreased (0.225766 --> 0.225603).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.225603 --> 0.225184).  Saving model ...\n",
      "Validation loss decreased (0.225184 --> 0.224974).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "50  epoch - train_loss:  0.225 , val_loss:  0.2252\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.224974 --> 0.224036).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.224036 --> 0.223679).  Saving model ...\n",
      "Validation loss decreased (0.223679 --> 0.223239).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.223239 --> 0.222845).  Saving model ...\n",
      "60  epoch - train_loss:  0.2236 , val_loss:  0.2256\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.222845 --> 0.222454).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.222454 --> 0.221656).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "70  epoch - train_loss:  0.2218 , val_loss:  0.222\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.221656 --> 0.221278).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.221278 --> 0.220566).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.220566 --> 0.220377).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "80  epoch - train_loss:  0.2204 , val_loss:  0.2213\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.220377 --> 0.219876).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.219876 --> 0.219795).  Saving model ...\n",
      "Validation loss decreased (0.219795 --> 0.219515).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.219515 --> 0.219450).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.219450 --> 0.219328).  Saving model ...\n",
      "90  epoch - train_loss:  0.2193 , val_loss:  0.2193\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.219328 --> 0.219169).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.219169 --> 0.218876).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.218876 --> 0.218410).  Saving model ...\n",
      "100  epoch - train_loss:  0.2188 , val_loss:  0.2189\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.218410 --> 0.218216).  Saving model ...\n",
      "Validation loss decreased (0.218216 --> 0.218128).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.218128 --> 0.217778).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.217778 --> 0.217630).  Saving model ...\n",
      "110  epoch - train_loss:  0.2176 , val_loss:  0.218\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.217630 --> 0.217451).  Saving model ...\n",
      "Validation loss decreased (0.217451 --> 0.217129).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.217129 --> 0.216681).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.216681 --> 0.216594).  Saving model ...\n",
      "120  epoch - train_loss:  0.2169 , val_loss:  0.2165\n",
      "Validation loss decreased (0.216594 --> 0.216494).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Validation loss decreased (0.216494 --> 0.216407).  Saving model ...\n",
      "130  epoch - train_loss:  0.2167 , val_loss:  0.2172\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.216407 --> 0.216178).  Saving model ...\n",
      "Validation loss decreased (0.216178 --> 0.215969).  Saving model ...\n",
      "Validation loss decreased (0.215969 --> 0.215791).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.215791 --> 0.215789).  Saving model ...\n",
      "Validation loss decreased (0.215789 --> 0.215700).  Saving model ...\n",
      "Validation loss decreased (0.215700 --> 0.215664).  Saving model ...\n",
      "140  epoch - train_loss:  0.2159 , val_loss:  0.2157\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.215664 --> 0.215648).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.215648 --> 0.215354).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "150  epoch - train_loss:  0.2152 , val_loss:  0.2153\n",
      "Validation loss decreased (0.215354 --> 0.215320).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.215320 --> 0.214984).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.214984 --> 0.214662).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.214662 --> 0.214537).  Saving model ...\n",
      "160  epoch - train_loss:  0.2151 , val_loss:  0.2151\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.214537 --> 0.214430).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.214430 --> 0.214393).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.214393 --> 0.214190).  Saving model ...\n",
      "170  epoch - train_loss:  0.2144 , val_loss:  0.2139\n",
      "Validation loss decreased (0.214190 --> 0.213856).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.213856 --> 0.213580).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "180  epoch - train_loss:  0.2141 , val_loss:  0.2141\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early Stopping!!\n",
      "fold:  4\n",
      "====================================================================================================\n",
      "Validation loss decreased (inf --> 0.413311).  Saving model ...\n",
      "Validation loss decreased (0.413311 --> 0.332534).  Saving model ...\n",
      "Validation loss decreased (0.332534 --> 0.296925).  Saving model ...\n",
      "Validation loss decreased (0.296925 --> 0.276611).  Saving model ...\n",
      "Validation loss decreased (0.276611 --> 0.266566).  Saving model ...\n",
      "Validation loss decreased (0.266566 --> 0.262606).  Saving model ...\n",
      "Validation loss decreased (0.262606 --> 0.254995).  Saving model ...\n",
      "Validation loss decreased (0.254995 --> 0.250369).  Saving model ...\n",
      "Validation loss decreased (0.250369 --> 0.247110).  Saving model ...\n",
      "10  epoch - train_loss:  0.247 , val_loss:  0.2463\n",
      "Validation loss decreased (0.247110 --> 0.246280).  Saving model ...\n",
      "Validation loss decreased (0.246280 --> 0.241791).  Saving model ...\n",
      "Validation loss decreased (0.241791 --> 0.241783).  Saving model ...\n",
      "Validation loss decreased (0.241783 --> 0.237212).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.237212 --> 0.234670).  Saving model ...\n",
      "Validation loss decreased (0.234670 --> 0.233549).  Saving model ...\n",
      "Validation loss decreased (0.233549 --> 0.233277).  Saving model ...\n",
      "Validation loss decreased (0.233277 --> 0.233243).  Saving model ...\n",
      "Validation loss decreased (0.233243 --> 0.231453).  Saving model ...\n",
      "20  epoch - train_loss:  0.231 , val_loss:  0.23\n",
      "Validation loss decreased (0.231453 --> 0.229954).  Saving model ...\n",
      "Validation loss decreased (0.229954 --> 0.228596).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.228596 --> 0.227342).  Saving model ...\n",
      "Validation loss decreased (0.227342 --> 0.226464).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.226464 --> 0.224848).  Saving model ...\n",
      "Validation loss decreased (0.224848 --> 0.224016).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "30  epoch - train_loss:  0.225 , val_loss:  0.224\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.224016 --> 0.223104).  Saving model ...\n",
      "Validation loss decreased (0.223104 --> 0.222449).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.222449 --> 0.220867).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.220867 --> 0.220660).  Saving model ...\n",
      "Validation loss decreased (0.220660 --> 0.220218).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "40  epoch - train_loss:  0.2213 , val_loss:  0.2193\n",
      "Validation loss decreased (0.220218 --> 0.219256).  Saving model ...\n",
      "Validation loss decreased (0.219256 --> 0.218805).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.218805 --> 0.218528).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.218528 --> 0.217927).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.217927 --> 0.217537).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "50  epoch - train_loss:  0.2179 , val_loss:  0.2166\n",
      "Validation loss decreased (0.217537 --> 0.216624).  Saving model ...\n",
      "Validation loss decreased (0.216624 --> 0.216515).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.216515 --> 0.215785).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.215785 --> 0.214577).  Saving model ...\n",
      "60  epoch - train_loss:  0.2163 , val_loss:  0.2154\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.214577 --> 0.214115).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.214115 --> 0.213205).  Saving model ...\n",
      "70  epoch - train_loss:  0.2147 , val_loss:  0.2137\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.213205 --> 0.213075).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.213075 --> 0.212860).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.212860 --> 0.212289).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "80  epoch - train_loss:  0.2132 , val_loss:  0.2121\n",
      "Validation loss decreased (0.212289 --> 0.212078).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.212078 --> 0.211753).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.211753 --> 0.211380).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.211380 --> 0.211042).  Saving model ...\n",
      "90  epoch - train_loss:  0.2119 , val_loss:  0.211\n",
      "Validation loss decreased (0.211042 --> 0.211034).  Saving model ...\n",
      "Validation loss decreased (0.211034 --> 0.210953).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.210953 --> 0.210820).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.210820 --> 0.210673).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.210673 --> 0.210386).  Saving model ...\n",
      "100  epoch - train_loss:  0.2114 , val_loss:  0.2103\n",
      "Validation loss decreased (0.210386 --> 0.210298).  Saving model ...\n",
      "Validation loss decreased (0.210298 --> 0.210015).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.210015 --> 0.209798).  Saving model ...\n",
      "Validation loss decreased (0.209798 --> 0.209380).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.209380 --> 0.209141).  Saving model ...\n",
      "110  epoch - train_loss:  0.2103 , val_loss:  0.2092\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.209141 --> 0.209134).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.209134 --> 0.208902).  Saving model ...\n",
      "Validation loss decreased (0.208902 --> 0.208795).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.208795 --> 0.208724).  Saving model ...\n",
      "120  epoch - train_loss:  0.2095 , val_loss:  0.2093\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Validation loss decreased (0.208724 --> 0.208126).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "130  epoch - train_loss:  0.2091 , val_loss:  0.2079\n",
      "Validation loss decreased (0.208126 --> 0.207932).  Saving model ...\n",
      "Validation loss decreased (0.207932 --> 0.207925).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.207925 --> 0.207839).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.207839 --> 0.207479).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.207479 --> 0.207469).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "140  epoch - train_loss:  0.2085 , val_loss:  0.2082\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.207469 --> 0.207245).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "Validation loss decreased (0.207245 --> 0.206936).  Saving model ...\n",
      "150  epoch - train_loss:  0.2077 , val_loss:  0.2074\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "Validation loss decreased (0.206936 --> 0.206598).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "160  epoch - train_loss:  0.2074 , val_loss:  0.2065\n",
      "Validation loss decreased (0.206598 --> 0.206533).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "Validation loss decreased (0.206533 --> 0.206382).  Saving model ...\n",
      "Validation loss decreased (0.206382 --> 0.206364).  Saving model ...\n",
      "Validation loss decreased (0.206364 --> 0.206245).  Saving model ...\n",
      "170  epoch - train_loss:  0.2073 , val_loss:  0.2068\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.206245 --> 0.206213).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "Validation loss decreased (0.206213 --> 0.206171).  Saving model ...\n",
      "Validation loss decreased (0.206171 --> 0.205914).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.205914 --> 0.205749).  Saving model ...\n",
      "180  epoch - train_loss:  0.207 , val_loss:  0.2058\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.205749 --> 0.205700).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "190  epoch - train_loss:  0.2065 , val_loss:  0.2057\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "Validation loss decreased (0.205700 --> 0.205586).  Saving model ...\n",
      "Validation loss decreased (0.205586 --> 0.205463).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "Validation loss decreased (0.205463 --> 0.205301).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "200  epoch - train_loss:  0.2061 , val_loss:  0.2051\n",
      "Validation loss decreased (0.205301 --> 0.205150).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "Validation loss decreased (0.205150 --> 0.205119).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "EarlyStopping counter: 3 out of 10\n",
      "Validation loss decreased (0.205119 --> 0.204890).  Saving model ...\n",
      "EarlyStopping counter: 1 out of 10\n",
      "EarlyStopping counter: 2 out of 10\n",
      "210  epoch - train_loss:  0.2059 , val_loss:  0.2053\n",
      "EarlyStopping counter: 3 out of 10\n",
      "EarlyStopping counter: 4 out of 10\n",
      "EarlyStopping counter: 5 out of 10\n",
      "EarlyStopping counter: 6 out of 10\n",
      "EarlyStopping counter: 7 out of 10\n",
      "EarlyStopping counter: 8 out of 10\n",
      "EarlyStopping counter: 9 out of 10\n",
      "EarlyStopping counter: 10 out of 10\n",
      "Early Stopping!!\n",
      "cv:  11.555\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=55)\n",
    "epochs = 10000\n",
    "\n",
    "cv = 0\n",
    "models = []\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_data)):\n",
    "    print('fold: ', fold)\n",
    "    print('='*100)\n",
    "    train_dataset = DataSet(train_data[train_idx].cuda())\n",
    "    val_dataset = DataSet(train_data[val_idx].cuda())\n",
    "    train_loader = DataLoader(train_dataset, 4096, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, 4096)\n",
    "    \n",
    "    model = denoising_model(train_data.shape[1]).cuda()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    earlystopping = EarlyStopping(patience=10, verbose=True, path=fm_path+'/checkpoint.pth')\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, val_loss = 0, 0\n",
    "\n",
    "        for j, data in enumerate(train_loader):\n",
    "            noise = torch.randn(data.shape).cuda()\n",
    "            dirty = data + noise\n",
    "            output = model(dirty)\n",
    "            loss = criterion(output, data)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * data.shape[0]\n",
    "        \n",
    "        train_loss /= len(train_dataset)\n",
    "        \n",
    "        for j, data in enumerate(val_loader):\n",
    "            noise = torch.randn(data.shape).cuda()\n",
    "            dirty = data + noise\n",
    "            output = model(dirty)\n",
    "            loss = criterion(output, data)\n",
    "            val_loss += loss.item() * data.shape[0]\n",
    "        \n",
    "        val_loss /= len(val_dataset)\n",
    "        if (i+1) % 10 == 0:\n",
    "            print(i+1, \" epoch - train_loss: \", round(train_loss, 4), \", val_loss: \", round(val_loss, 4))\n",
    "        earlystopping(val_loss, model)\n",
    "        if earlystopping.early_stop:\n",
    "            print(\"Early Stopping!!\")\n",
    "            break\n",
    "    cv += val_loss\n",
    "    model.load_state_dict(torch.load(fm_path+'/checkpoint.pth'))\n",
    "    models.append(model)\n",
    "cv /= 5\n",
    "print(\"cv: \", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.zeros((train_data.shape[0], 128))\n",
    "for i, model in enumerate(models):\n",
    "    # train_dataset = DataSet(train_data)\n",
    "    # train_loader = DataLoader(train_dataset, 4096, shuffle=False)\n",
    "    # for j, data in enumerate(train_loader):\n",
    "    output += model.encode(train_data).cpu() / 5\n",
    "    torch.save(model.state_dict(), fm_path+'/model-'+str(i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.4571, 1.2547, 1.9901,  ..., 2.6923, 1.7736, 0.6860],\n",
       "        [1.3585, 0.8547, 1.8584,  ..., 2.7255, 1.8111, 0.5490],\n",
       "        [1.3632, 1.1603, 2.0882,  ..., 2.9494, 1.4200, 1.2114],\n",
       "        ...,\n",
       "        [3.2910, 1.8508, 4.6874,  ..., 2.9970, 3.4341, 2.3833],\n",
       "        [3.0671, 1.9353, 4.5361,  ..., 2.8554, 2.9525, 2.6529],\n",
       "        [3.1853, 1.4961, 4.5273,  ..., 2.9214, 3.0446, 2.6488]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([428932, 128])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAE_0</th>\n",
       "      <th>DAE_1</th>\n",
       "      <th>DAE_2</th>\n",
       "      <th>DAE_3</th>\n",
       "      <th>DAE_4</th>\n",
       "      <th>DAE_5</th>\n",
       "      <th>DAE_6</th>\n",
       "      <th>DAE_7</th>\n",
       "      <th>DAE_8</th>\n",
       "      <th>DAE_9</th>\n",
       "      <th>DAE_10</th>\n",
       "      <th>DAE_11</th>\n",
       "      <th>DAE_12</th>\n",
       "      <th>DAE_13</th>\n",
       "      <th>DAE_14</th>\n",
       "      <th>DAE_15</th>\n",
       "      <th>DAE_16</th>\n",
       "      <th>DAE_17</th>\n",
       "      <th>DAE_18</th>\n",
       "      <th>DAE_19</th>\n",
       "      <th>DAE_20</th>\n",
       "      <th>DAE_21</th>\n",
       "      <th>DAE_22</th>\n",
       "      <th>DAE_23</th>\n",
       "      <th>DAE_24</th>\n",
       "      <th>DAE_25</th>\n",
       "      <th>DAE_26</th>\n",
       "      <th>DAE_27</th>\n",
       "      <th>DAE_28</th>\n",
       "      <th>DAE_29</th>\n",
       "      <th>DAE_30</th>\n",
       "      <th>DAE_31</th>\n",
       "      <th>DAE_32</th>\n",
       "      <th>DAE_33</th>\n",
       "      <th>DAE_34</th>\n",
       "      <th>DAE_35</th>\n",
       "      <th>DAE_36</th>\n",
       "      <th>DAE_37</th>\n",
       "      <th>DAE_38</th>\n",
       "      <th>DAE_39</th>\n",
       "      <th>DAE_40</th>\n",
       "      <th>DAE_41</th>\n",
       "      <th>DAE_42</th>\n",
       "      <th>DAE_43</th>\n",
       "      <th>DAE_44</th>\n",
       "      <th>DAE_45</th>\n",
       "      <th>DAE_46</th>\n",
       "      <th>DAE_47</th>\n",
       "      <th>DAE_48</th>\n",
       "      <th>DAE_49</th>\n",
       "      <th>DAE_50</th>\n",
       "      <th>DAE_51</th>\n",
       "      <th>DAE_52</th>\n",
       "      <th>DAE_53</th>\n",
       "      <th>DAE_54</th>\n",
       "      <th>DAE_55</th>\n",
       "      <th>DAE_56</th>\n",
       "      <th>DAE_57</th>\n",
       "      <th>DAE_58</th>\n",
       "      <th>DAE_59</th>\n",
       "      <th>DAE_60</th>\n",
       "      <th>DAE_61</th>\n",
       "      <th>DAE_62</th>\n",
       "      <th>DAE_63</th>\n",
       "      <th>DAE_64</th>\n",
       "      <th>DAE_65</th>\n",
       "      <th>DAE_66</th>\n",
       "      <th>DAE_67</th>\n",
       "      <th>DAE_68</th>\n",
       "      <th>DAE_69</th>\n",
       "      <th>DAE_70</th>\n",
       "      <th>DAE_71</th>\n",
       "      <th>DAE_72</th>\n",
       "      <th>DAE_73</th>\n",
       "      <th>DAE_74</th>\n",
       "      <th>DAE_75</th>\n",
       "      <th>DAE_76</th>\n",
       "      <th>DAE_77</th>\n",
       "      <th>DAE_78</th>\n",
       "      <th>DAE_79</th>\n",
       "      <th>DAE_80</th>\n",
       "      <th>DAE_81</th>\n",
       "      <th>DAE_82</th>\n",
       "      <th>DAE_83</th>\n",
       "      <th>DAE_84</th>\n",
       "      <th>DAE_85</th>\n",
       "      <th>DAE_86</th>\n",
       "      <th>DAE_87</th>\n",
       "      <th>DAE_88</th>\n",
       "      <th>DAE_89</th>\n",
       "      <th>DAE_90</th>\n",
       "      <th>DAE_91</th>\n",
       "      <th>DAE_92</th>\n",
       "      <th>DAE_93</th>\n",
       "      <th>DAE_94</th>\n",
       "      <th>DAE_95</th>\n",
       "      <th>DAE_96</th>\n",
       "      <th>DAE_97</th>\n",
       "      <th>DAE_98</th>\n",
       "      <th>DAE_99</th>\n",
       "      <th>DAE_100</th>\n",
       "      <th>DAE_101</th>\n",
       "      <th>DAE_102</th>\n",
       "      <th>DAE_103</th>\n",
       "      <th>DAE_104</th>\n",
       "      <th>DAE_105</th>\n",
       "      <th>DAE_106</th>\n",
       "      <th>DAE_107</th>\n",
       "      <th>DAE_108</th>\n",
       "      <th>DAE_109</th>\n",
       "      <th>DAE_110</th>\n",
       "      <th>DAE_111</th>\n",
       "      <th>DAE_112</th>\n",
       "      <th>DAE_113</th>\n",
       "      <th>DAE_114</th>\n",
       "      <th>DAE_115</th>\n",
       "      <th>DAE_116</th>\n",
       "      <th>DAE_117</th>\n",
       "      <th>DAE_118</th>\n",
       "      <th>DAE_119</th>\n",
       "      <th>DAE_120</th>\n",
       "      <th>DAE_121</th>\n",
       "      <th>DAE_122</th>\n",
       "      <th>DAE_123</th>\n",
       "      <th>DAE_124</th>\n",
       "      <th>DAE_125</th>\n",
       "      <th>DAE_126</th>\n",
       "      <th>DAE_127</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "      <td>428932.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.024674</td>\n",
       "      <td>1.408132</td>\n",
       "      <td>2.937426</td>\n",
       "      <td>2.922235</td>\n",
       "      <td>1.404105</td>\n",
       "      <td>2.793133</td>\n",
       "      <td>2.509424</td>\n",
       "      <td>0.836562</td>\n",
       "      <td>3.257326</td>\n",
       "      <td>3.626128</td>\n",
       "      <td>1.956815</td>\n",
       "      <td>1.648261</td>\n",
       "      <td>2.811153</td>\n",
       "      <td>3.208830</td>\n",
       "      <td>1.484894</td>\n",
       "      <td>2.376671</td>\n",
       "      <td>3.860445</td>\n",
       "      <td>1.679050</td>\n",
       "      <td>1.981682</td>\n",
       "      <td>1.661340</td>\n",
       "      <td>2.739988</td>\n",
       "      <td>1.137162</td>\n",
       "      <td>2.801373</td>\n",
       "      <td>2.568943</td>\n",
       "      <td>3.491256</td>\n",
       "      <td>4.054435</td>\n",
       "      <td>1.839545</td>\n",
       "      <td>2.412157</td>\n",
       "      <td>2.417036</td>\n",
       "      <td>0.990277</td>\n",
       "      <td>0.898654</td>\n",
       "      <td>1.914059</td>\n",
       "      <td>2.411544</td>\n",
       "      <td>2.159241</td>\n",
       "      <td>2.735686</td>\n",
       "      <td>2.314098</td>\n",
       "      <td>1.951012</td>\n",
       "      <td>1.348031</td>\n",
       "      <td>3.207363</td>\n",
       "      <td>2.653598</td>\n",
       "      <td>1.316775</td>\n",
       "      <td>3.112838</td>\n",
       "      <td>1.679902</td>\n",
       "      <td>3.475783</td>\n",
       "      <td>2.062476</td>\n",
       "      <td>0.473169</td>\n",
       "      <td>0.911093</td>\n",
       "      <td>2.108604</td>\n",
       "      <td>1.854478</td>\n",
       "      <td>2.301869</td>\n",
       "      <td>3.907174</td>\n",
       "      <td>2.412498</td>\n",
       "      <td>2.093194</td>\n",
       "      <td>2.144174</td>\n",
       "      <td>2.688028</td>\n",
       "      <td>2.562392</td>\n",
       "      <td>0.542321</td>\n",
       "      <td>2.804337</td>\n",
       "      <td>4.070835</td>\n",
       "      <td>2.631412</td>\n",
       "      <td>3.579354</td>\n",
       "      <td>0.839137</td>\n",
       "      <td>2.388687</td>\n",
       "      <td>3.143724</td>\n",
       "      <td>1.632429</td>\n",
       "      <td>2.864942</td>\n",
       "      <td>5.138174</td>\n",
       "      <td>3.223232</td>\n",
       "      <td>1.306428</td>\n",
       "      <td>3.053071</td>\n",
       "      <td>0.973733</td>\n",
       "      <td>1.451537</td>\n",
       "      <td>0.411263</td>\n",
       "      <td>2.231162</td>\n",
       "      <td>2.314613</td>\n",
       "      <td>2.731990</td>\n",
       "      <td>1.031546</td>\n",
       "      <td>2.230681</td>\n",
       "      <td>3.490454</td>\n",
       "      <td>1.545617</td>\n",
       "      <td>2.844125</td>\n",
       "      <td>2.217495</td>\n",
       "      <td>2.502552</td>\n",
       "      <td>0.874606</td>\n",
       "      <td>1.035058</td>\n",
       "      <td>4.537579</td>\n",
       "      <td>1.466348</td>\n",
       "      <td>2.935630</td>\n",
       "      <td>2.836405</td>\n",
       "      <td>1.596715</td>\n",
       "      <td>2.341712</td>\n",
       "      <td>1.807350</td>\n",
       "      <td>2.057317</td>\n",
       "      <td>2.665547</td>\n",
       "      <td>3.989386</td>\n",
       "      <td>3.086629</td>\n",
       "      <td>3.927931</td>\n",
       "      <td>1.440984</td>\n",
       "      <td>2.123539</td>\n",
       "      <td>1.637443</td>\n",
       "      <td>2.927938</td>\n",
       "      <td>2.510748</td>\n",
       "      <td>1.107941</td>\n",
       "      <td>3.701484</td>\n",
       "      <td>4.635833</td>\n",
       "      <td>0.525721</td>\n",
       "      <td>1.134774</td>\n",
       "      <td>2.043727</td>\n",
       "      <td>3.750197</td>\n",
       "      <td>3.535721</td>\n",
       "      <td>1.566113</td>\n",
       "      <td>1.331328</td>\n",
       "      <td>2.827906</td>\n",
       "      <td>1.259785</td>\n",
       "      <td>3.121304</td>\n",
       "      <td>1.359910</td>\n",
       "      <td>1.340247</td>\n",
       "      <td>3.293372</td>\n",
       "      <td>4.014331</td>\n",
       "      <td>2.090719</td>\n",
       "      <td>1.842517</td>\n",
       "      <td>1.283807</td>\n",
       "      <td>3.295710</td>\n",
       "      <td>2.128874</td>\n",
       "      <td>2.004336</td>\n",
       "      <td>2.287117</td>\n",
       "      <td>1.753305</td>\n",
       "      <td>1.867003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.017644</td>\n",
       "      <td>0.951307</td>\n",
       "      <td>1.306224</td>\n",
       "      <td>1.491263</td>\n",
       "      <td>0.769466</td>\n",
       "      <td>1.364367</td>\n",
       "      <td>1.119864</td>\n",
       "      <td>0.786350</td>\n",
       "      <td>1.459839</td>\n",
       "      <td>1.641880</td>\n",
       "      <td>1.332849</td>\n",
       "      <td>0.882851</td>\n",
       "      <td>1.600111</td>\n",
       "      <td>1.297801</td>\n",
       "      <td>0.693050</td>\n",
       "      <td>0.802069</td>\n",
       "      <td>1.967745</td>\n",
       "      <td>0.899305</td>\n",
       "      <td>1.037621</td>\n",
       "      <td>0.839485</td>\n",
       "      <td>1.533602</td>\n",
       "      <td>1.220607</td>\n",
       "      <td>1.484159</td>\n",
       "      <td>1.360320</td>\n",
       "      <td>1.689755</td>\n",
       "      <td>1.931580</td>\n",
       "      <td>0.983720</td>\n",
       "      <td>1.196447</td>\n",
       "      <td>1.160527</td>\n",
       "      <td>0.896995</td>\n",
       "      <td>0.690273</td>\n",
       "      <td>0.717938</td>\n",
       "      <td>1.295802</td>\n",
       "      <td>1.374520</td>\n",
       "      <td>1.280010</td>\n",
       "      <td>1.200422</td>\n",
       "      <td>0.975409</td>\n",
       "      <td>0.463264</td>\n",
       "      <td>1.424147</td>\n",
       "      <td>0.825631</td>\n",
       "      <td>0.728766</td>\n",
       "      <td>1.511979</td>\n",
       "      <td>0.897245</td>\n",
       "      <td>1.939461</td>\n",
       "      <td>1.334288</td>\n",
       "      <td>0.383691</td>\n",
       "      <td>0.545298</td>\n",
       "      <td>1.108517</td>\n",
       "      <td>0.976878</td>\n",
       "      <td>1.215984</td>\n",
       "      <td>2.184227</td>\n",
       "      <td>0.924319</td>\n",
       "      <td>0.891804</td>\n",
       "      <td>1.138509</td>\n",
       "      <td>1.355214</td>\n",
       "      <td>1.293900</td>\n",
       "      <td>0.511135</td>\n",
       "      <td>1.157645</td>\n",
       "      <td>1.971765</td>\n",
       "      <td>1.186085</td>\n",
       "      <td>2.114194</td>\n",
       "      <td>0.642434</td>\n",
       "      <td>1.161383</td>\n",
       "      <td>1.706791</td>\n",
       "      <td>1.047712</td>\n",
       "      <td>1.429129</td>\n",
       "      <td>3.182129</td>\n",
       "      <td>1.502650</td>\n",
       "      <td>0.901202</td>\n",
       "      <td>1.649843</td>\n",
       "      <td>0.700494</td>\n",
       "      <td>0.570764</td>\n",
       "      <td>0.589004</td>\n",
       "      <td>0.948936</td>\n",
       "      <td>1.210597</td>\n",
       "      <td>1.108601</td>\n",
       "      <td>1.135030</td>\n",
       "      <td>0.820134</td>\n",
       "      <td>2.186010</td>\n",
       "      <td>0.730072</td>\n",
       "      <td>1.240039</td>\n",
       "      <td>1.463566</td>\n",
       "      <td>1.101996</td>\n",
       "      <td>0.394449</td>\n",
       "      <td>0.858471</td>\n",
       "      <td>1.737618</td>\n",
       "      <td>0.901441</td>\n",
       "      <td>1.402081</td>\n",
       "      <td>1.082996</td>\n",
       "      <td>0.724913</td>\n",
       "      <td>0.810419</td>\n",
       "      <td>0.697783</td>\n",
       "      <td>0.996474</td>\n",
       "      <td>0.818546</td>\n",
       "      <td>1.682098</td>\n",
       "      <td>1.141823</td>\n",
       "      <td>1.934143</td>\n",
       "      <td>1.159958</td>\n",
       "      <td>1.182307</td>\n",
       "      <td>0.881222</td>\n",
       "      <td>1.357506</td>\n",
       "      <td>1.256620</td>\n",
       "      <td>1.149014</td>\n",
       "      <td>1.779679</td>\n",
       "      <td>2.497866</td>\n",
       "      <td>0.577746</td>\n",
       "      <td>0.657375</td>\n",
       "      <td>0.918154</td>\n",
       "      <td>1.617673</td>\n",
       "      <td>1.759838</td>\n",
       "      <td>0.587936</td>\n",
       "      <td>0.739148</td>\n",
       "      <td>1.285038</td>\n",
       "      <td>0.988571</td>\n",
       "      <td>1.126258</td>\n",
       "      <td>0.787080</td>\n",
       "      <td>0.931314</td>\n",
       "      <td>1.330197</td>\n",
       "      <td>1.756180</td>\n",
       "      <td>1.174962</td>\n",
       "      <td>0.873349</td>\n",
       "      <td>1.243757</td>\n",
       "      <td>1.718334</td>\n",
       "      <td>0.978994</td>\n",
       "      <td>0.886139</td>\n",
       "      <td>0.729555</td>\n",
       "      <td>0.748291</td>\n",
       "      <td>0.698339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.062018</td>\n",
       "      <td>-0.027860</td>\n",
       "      <td>0.812468</td>\n",
       "      <td>-0.017151</td>\n",
       "      <td>-0.026841</td>\n",
       "      <td>0.413867</td>\n",
       "      <td>-0.107802</td>\n",
       "      <td>-0.043650</td>\n",
       "      <td>0.407278</td>\n",
       "      <td>0.543840</td>\n",
       "      <td>0.428674</td>\n",
       "      <td>0.009789</td>\n",
       "      <td>0.005310</td>\n",
       "      <td>0.384764</td>\n",
       "      <td>0.260447</td>\n",
       "      <td>0.561378</td>\n",
       "      <td>0.375714</td>\n",
       "      <td>0.428371</td>\n",
       "      <td>0.042687</td>\n",
       "      <td>-0.126296</td>\n",
       "      <td>-0.026918</td>\n",
       "      <td>-0.087905</td>\n",
       "      <td>0.208290</td>\n",
       "      <td>0.238699</td>\n",
       "      <td>0.296108</td>\n",
       "      <td>0.555157</td>\n",
       "      <td>0.144452</td>\n",
       "      <td>0.216937</td>\n",
       "      <td>-0.074775</td>\n",
       "      <td>-0.193840</td>\n",
       "      <td>-0.185030</td>\n",
       "      <td>0.496002</td>\n",
       "      <td>0.011655</td>\n",
       "      <td>-0.202392</td>\n",
       "      <td>0.031893</td>\n",
       "      <td>0.503008</td>\n",
       "      <td>0.424149</td>\n",
       "      <td>-0.133708</td>\n",
       "      <td>0.626102</td>\n",
       "      <td>0.554607</td>\n",
       "      <td>-0.091645</td>\n",
       "      <td>0.257875</td>\n",
       "      <td>0.469841</td>\n",
       "      <td>0.014262</td>\n",
       "      <td>0.167054</td>\n",
       "      <td>-0.169340</td>\n",
       "      <td>-0.061915</td>\n",
       "      <td>0.316752</td>\n",
       "      <td>-0.034165</td>\n",
       "      <td>0.728749</td>\n",
       "      <td>0.165143</td>\n",
       "      <td>0.533683</td>\n",
       "      <td>0.268449</td>\n",
       "      <td>0.171979</td>\n",
       "      <td>0.017963</td>\n",
       "      <td>0.181411</td>\n",
       "      <td>-0.198090</td>\n",
       "      <td>0.564466</td>\n",
       "      <td>0.237165</td>\n",
       "      <td>0.425648</td>\n",
       "      <td>-0.063608</td>\n",
       "      <td>-0.127744</td>\n",
       "      <td>0.207885</td>\n",
       "      <td>0.154899</td>\n",
       "      <td>0.330341</td>\n",
       "      <td>0.412181</td>\n",
       "      <td>-0.071040</td>\n",
       "      <td>0.522588</td>\n",
       "      <td>0.139419</td>\n",
       "      <td>-0.105705</td>\n",
       "      <td>-0.165483</td>\n",
       "      <td>-0.135702</td>\n",
       "      <td>-0.238229</td>\n",
       "      <td>0.335615</td>\n",
       "      <td>0.121074</td>\n",
       "      <td>0.733622</td>\n",
       "      <td>-0.223446</td>\n",
       "      <td>0.040887</td>\n",
       "      <td>-0.110965</td>\n",
       "      <td>0.060668</td>\n",
       "      <td>0.162604</td>\n",
       "      <td>-0.022150</td>\n",
       "      <td>0.490635</td>\n",
       "      <td>-0.148190</td>\n",
       "      <td>-0.103424</td>\n",
       "      <td>1.658968</td>\n",
       "      <td>0.222788</td>\n",
       "      <td>0.346246</td>\n",
       "      <td>0.520826</td>\n",
       "      <td>0.294630</td>\n",
       "      <td>0.515861</td>\n",
       "      <td>0.408061</td>\n",
       "      <td>0.029962</td>\n",
       "      <td>1.165267</td>\n",
       "      <td>0.690166</td>\n",
       "      <td>1.097157</td>\n",
       "      <td>0.406276</td>\n",
       "      <td>-0.223543</td>\n",
       "      <td>0.114398</td>\n",
       "      <td>0.034191</td>\n",
       "      <td>-0.103880</td>\n",
       "      <td>-0.037165</td>\n",
       "      <td>-0.163817</td>\n",
       "      <td>0.524843</td>\n",
       "      <td>0.522679</td>\n",
       "      <td>-0.193335</td>\n",
       "      <td>0.046562</td>\n",
       "      <td>0.129730</td>\n",
       "      <td>0.644681</td>\n",
       "      <td>0.512510</td>\n",
       "      <td>0.019789</td>\n",
       "      <td>0.183774</td>\n",
       "      <td>0.383844</td>\n",
       "      <td>-0.138195</td>\n",
       "      <td>0.060404</td>\n",
       "      <td>0.166807</td>\n",
       "      <td>-0.034977</td>\n",
       "      <td>0.842711</td>\n",
       "      <td>0.526916</td>\n",
       "      <td>-0.187488</td>\n",
       "      <td>0.219643</td>\n",
       "      <td>-0.024299</td>\n",
       "      <td>0.127327</td>\n",
       "      <td>0.236253</td>\n",
       "      <td>0.017489</td>\n",
       "      <td>0.418045</td>\n",
       "      <td>0.155278</td>\n",
       "      <td>0.471288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.274684</td>\n",
       "      <td>0.785676</td>\n",
       "      <td>1.880312</td>\n",
       "      <td>1.679957</td>\n",
       "      <td>0.818609</td>\n",
       "      <td>1.780044</td>\n",
       "      <td>1.592092</td>\n",
       "      <td>0.425845</td>\n",
       "      <td>2.094150</td>\n",
       "      <td>2.173276</td>\n",
       "      <td>1.199851</td>\n",
       "      <td>0.996286</td>\n",
       "      <td>1.575969</td>\n",
       "      <td>2.263134</td>\n",
       "      <td>1.004348</td>\n",
       "      <td>1.771035</td>\n",
       "      <td>2.307271</td>\n",
       "      <td>1.047377</td>\n",
       "      <td>1.173922</td>\n",
       "      <td>1.087964</td>\n",
       "      <td>1.556829</td>\n",
       "      <td>0.332557</td>\n",
       "      <td>1.569570</td>\n",
       "      <td>1.570461</td>\n",
       "      <td>2.089021</td>\n",
       "      <td>2.467633</td>\n",
       "      <td>1.042034</td>\n",
       "      <td>1.388354</td>\n",
       "      <td>1.617658</td>\n",
       "      <td>0.300286</td>\n",
       "      <td>0.371273</td>\n",
       "      <td>1.388170</td>\n",
       "      <td>1.390193</td>\n",
       "      <td>1.114027</td>\n",
       "      <td>1.639560</td>\n",
       "      <td>1.523963</td>\n",
       "      <td>1.311072</td>\n",
       "      <td>1.083040</td>\n",
       "      <td>2.038432</td>\n",
       "      <td>2.012628</td>\n",
       "      <td>0.785273</td>\n",
       "      <td>1.942211</td>\n",
       "      <td>1.123655</td>\n",
       "      <td>1.885470</td>\n",
       "      <td>1.025364</td>\n",
       "      <td>0.178673</td>\n",
       "      <td>0.529758</td>\n",
       "      <td>1.286473</td>\n",
       "      <td>1.170368</td>\n",
       "      <td>1.611055</td>\n",
       "      <td>2.079033</td>\n",
       "      <td>1.692532</td>\n",
       "      <td>1.404396</td>\n",
       "      <td>1.185068</td>\n",
       "      <td>1.598881</td>\n",
       "      <td>1.625813</td>\n",
       "      <td>0.204318</td>\n",
       "      <td>1.878807</td>\n",
       "      <td>2.343534</td>\n",
       "      <td>1.771640</td>\n",
       "      <td>1.717301</td>\n",
       "      <td>0.369828</td>\n",
       "      <td>1.607864</td>\n",
       "      <td>1.654081</td>\n",
       "      <td>1.023143</td>\n",
       "      <td>1.806286</td>\n",
       "      <td>2.388594</td>\n",
       "      <td>1.924301</td>\n",
       "      <td>0.805316</td>\n",
       "      <td>1.677071</td>\n",
       "      <td>0.491612</td>\n",
       "      <td>1.074723</td>\n",
       "      <td>-0.027381</td>\n",
       "      <td>1.455519</td>\n",
       "      <td>1.415537</td>\n",
       "      <td>1.881561</td>\n",
       "      <td>0.212787</td>\n",
       "      <td>1.573933</td>\n",
       "      <td>1.508174</td>\n",
       "      <td>1.000390</td>\n",
       "      <td>1.886210</td>\n",
       "      <td>1.089769</td>\n",
       "      <td>1.688613</td>\n",
       "      <td>0.636378</td>\n",
       "      <td>0.478779</td>\n",
       "      <td>3.043893</td>\n",
       "      <td>0.938102</td>\n",
       "      <td>1.788649</td>\n",
       "      <td>2.082508</td>\n",
       "      <td>1.084822</td>\n",
       "      <td>1.712558</td>\n",
       "      <td>1.264002</td>\n",
       "      <td>1.286618</td>\n",
       "      <td>2.085054</td>\n",
       "      <td>2.641004</td>\n",
       "      <td>2.131509</td>\n",
       "      <td>2.363745</td>\n",
       "      <td>0.687904</td>\n",
       "      <td>1.334411</td>\n",
       "      <td>0.922157</td>\n",
       "      <td>1.899177</td>\n",
       "      <td>1.418818</td>\n",
       "      <td>0.326800</td>\n",
       "      <td>1.998526</td>\n",
       "      <td>2.311509</td>\n",
       "      <td>0.162656</td>\n",
       "      <td>0.761880</td>\n",
       "      <td>1.360384</td>\n",
       "      <td>2.209044</td>\n",
       "      <td>1.993971</td>\n",
       "      <td>1.214948</td>\n",
       "      <td>0.835670</td>\n",
       "      <td>1.837018</td>\n",
       "      <td>0.490853</td>\n",
       "      <td>2.302200</td>\n",
       "      <td>0.884167</td>\n",
       "      <td>0.751478</td>\n",
       "      <td>2.205729</td>\n",
       "      <td>2.557132</td>\n",
       "      <td>1.258192</td>\n",
       "      <td>1.128842</td>\n",
       "      <td>0.519243</td>\n",
       "      <td>1.866449</td>\n",
       "      <td>1.352503</td>\n",
       "      <td>1.392505</td>\n",
       "      <td>1.768940</td>\n",
       "      <td>1.197701</td>\n",
       "      <td>1.392847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.903999</td>\n",
       "      <td>1.213563</td>\n",
       "      <td>2.695305</td>\n",
       "      <td>2.857519</td>\n",
       "      <td>1.307921</td>\n",
       "      <td>2.602454</td>\n",
       "      <td>2.382053</td>\n",
       "      <td>0.670372</td>\n",
       "      <td>3.009487</td>\n",
       "      <td>3.387092</td>\n",
       "      <td>1.621315</td>\n",
       "      <td>1.562461</td>\n",
       "      <td>2.672050</td>\n",
       "      <td>3.129374</td>\n",
       "      <td>1.344065</td>\n",
       "      <td>2.274192</td>\n",
       "      <td>3.604648</td>\n",
       "      <td>1.538631</td>\n",
       "      <td>1.930358</td>\n",
       "      <td>1.532042</td>\n",
       "      <td>2.713564</td>\n",
       "      <td>0.840364</td>\n",
       "      <td>2.743024</td>\n",
       "      <td>2.386667</td>\n",
       "      <td>3.336596</td>\n",
       "      <td>4.010700</td>\n",
       "      <td>1.742618</td>\n",
       "      <td>2.298470</td>\n",
       "      <td>2.378002</td>\n",
       "      <td>0.876855</td>\n",
       "      <td>0.774970</td>\n",
       "      <td>1.765796</td>\n",
       "      <td>2.320869</td>\n",
       "      <td>2.010129</td>\n",
       "      <td>2.597027</td>\n",
       "      <td>2.073592</td>\n",
       "      <td>1.844840</td>\n",
       "      <td>1.268389</td>\n",
       "      <td>3.099233</td>\n",
       "      <td>2.536582</td>\n",
       "      <td>1.237081</td>\n",
       "      <td>2.995427</td>\n",
       "      <td>1.488312</td>\n",
       "      <td>3.342380</td>\n",
       "      <td>1.793241</td>\n",
       "      <td>0.366063</td>\n",
       "      <td>0.787764</td>\n",
       "      <td>1.947137</td>\n",
       "      <td>1.761132</td>\n",
       "      <td>2.045162</td>\n",
       "      <td>3.674017</td>\n",
       "      <td>2.281202</td>\n",
       "      <td>1.987781</td>\n",
       "      <td>2.055080</td>\n",
       "      <td>2.742975</td>\n",
       "      <td>2.437018</td>\n",
       "      <td>0.472236</td>\n",
       "      <td>2.701438</td>\n",
       "      <td>3.912577</td>\n",
       "      <td>2.448956</td>\n",
       "      <td>3.539217</td>\n",
       "      <td>0.703858</td>\n",
       "      <td>2.210787</td>\n",
       "      <td>3.017775</td>\n",
       "      <td>1.361162</td>\n",
       "      <td>2.649114</td>\n",
       "      <td>5.012832</td>\n",
       "      <td>3.074674</td>\n",
       "      <td>1.132413</td>\n",
       "      <td>2.928179</td>\n",
       "      <td>0.858776</td>\n",
       "      <td>1.347187</td>\n",
       "      <td>0.218637</td>\n",
       "      <td>2.169994</td>\n",
       "      <td>2.185149</td>\n",
       "      <td>2.533827</td>\n",
       "      <td>0.692259</td>\n",
       "      <td>2.032061</td>\n",
       "      <td>3.251299</td>\n",
       "      <td>1.535620</td>\n",
       "      <td>2.771674</td>\n",
       "      <td>2.049525</td>\n",
       "      <td>2.401886</td>\n",
       "      <td>0.798529</td>\n",
       "      <td>0.907009</td>\n",
       "      <td>4.226443</td>\n",
       "      <td>1.280626</td>\n",
       "      <td>2.825978</td>\n",
       "      <td>2.702868</td>\n",
       "      <td>1.484420</td>\n",
       "      <td>2.249470</td>\n",
       "      <td>1.751781</td>\n",
       "      <td>2.032765</td>\n",
       "      <td>2.512692</td>\n",
       "      <td>3.829549</td>\n",
       "      <td>2.932856</td>\n",
       "      <td>3.744698</td>\n",
       "      <td>1.293214</td>\n",
       "      <td>1.976575</td>\n",
       "      <td>1.541849</td>\n",
       "      <td>2.837799</td>\n",
       "      <td>2.649225</td>\n",
       "      <td>0.717668</td>\n",
       "      <td>3.611367</td>\n",
       "      <td>4.505921</td>\n",
       "      <td>0.361123</td>\n",
       "      <td>0.980042</td>\n",
       "      <td>2.021122</td>\n",
       "      <td>3.601254</td>\n",
       "      <td>3.388418</td>\n",
       "      <td>1.532064</td>\n",
       "      <td>1.182330</td>\n",
       "      <td>2.772798</td>\n",
       "      <td>1.066043</td>\n",
       "      <td>3.104760</td>\n",
       "      <td>1.203931</td>\n",
       "      <td>1.214430</td>\n",
       "      <td>3.117557</td>\n",
       "      <td>3.994386</td>\n",
       "      <td>1.973584</td>\n",
       "      <td>1.644768</td>\n",
       "      <td>0.856074</td>\n",
       "      <td>3.288159</td>\n",
       "      <td>2.051226</td>\n",
       "      <td>2.061236</td>\n",
       "      <td>2.236703</td>\n",
       "      <td>1.635326</td>\n",
       "      <td>1.822362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.549024</td>\n",
       "      <td>1.737550</td>\n",
       "      <td>3.884579</td>\n",
       "      <td>4.069175</td>\n",
       "      <td>1.938737</td>\n",
       "      <td>3.447420</td>\n",
       "      <td>3.468974</td>\n",
       "      <td>1.018479</td>\n",
       "      <td>4.554482</td>\n",
       "      <td>5.058271</td>\n",
       "      <td>2.203326</td>\n",
       "      <td>2.154535</td>\n",
       "      <td>3.995942</td>\n",
       "      <td>4.280066</td>\n",
       "      <td>1.809230</td>\n",
       "      <td>2.897684</td>\n",
       "      <td>5.609864</td>\n",
       "      <td>2.103645</td>\n",
       "      <td>2.668835</td>\n",
       "      <td>2.080106</td>\n",
       "      <td>3.823456</td>\n",
       "      <td>1.669661</td>\n",
       "      <td>3.917695</td>\n",
       "      <td>3.408428</td>\n",
       "      <td>4.821830</td>\n",
       "      <td>5.719940</td>\n",
       "      <td>2.491667</td>\n",
       "      <td>3.422202</td>\n",
       "      <td>3.212009</td>\n",
       "      <td>1.454160</td>\n",
       "      <td>1.361463</td>\n",
       "      <td>2.366352</td>\n",
       "      <td>3.420475</td>\n",
       "      <td>2.952448</td>\n",
       "      <td>3.853345</td>\n",
       "      <td>2.790218</td>\n",
       "      <td>2.371203</td>\n",
       "      <td>1.510947</td>\n",
       "      <td>4.200063</td>\n",
       "      <td>3.229191</td>\n",
       "      <td>1.829378</td>\n",
       "      <td>4.152659</td>\n",
       "      <td>1.966339</td>\n",
       "      <td>5.220947</td>\n",
       "      <td>2.836496</td>\n",
       "      <td>0.700508</td>\n",
       "      <td>1.175785</td>\n",
       "      <td>2.720881</td>\n",
       "      <td>2.417540</td>\n",
       "      <td>2.575374</td>\n",
       "      <td>5.706619</td>\n",
       "      <td>2.985958</td>\n",
       "      <td>2.711728</td>\n",
       "      <td>2.968689</td>\n",
       "      <td>3.683208</td>\n",
       "      <td>3.278931</td>\n",
       "      <td>0.752838</td>\n",
       "      <td>3.697036</td>\n",
       "      <td>5.807392</td>\n",
       "      <td>3.338212</td>\n",
       "      <td>5.447196</td>\n",
       "      <td>1.166043</td>\n",
       "      <td>3.021713</td>\n",
       "      <td>4.654884</td>\n",
       "      <td>1.904300</td>\n",
       "      <td>3.828641</td>\n",
       "      <td>8.047491</td>\n",
       "      <td>4.484698</td>\n",
       "      <td>1.536645</td>\n",
       "      <td>4.429292</td>\n",
       "      <td>1.284847</td>\n",
       "      <td>1.782634</td>\n",
       "      <td>0.629469</td>\n",
       "      <td>2.975945</td>\n",
       "      <td>3.028923</td>\n",
       "      <td>3.476605</td>\n",
       "      <td>1.599723</td>\n",
       "      <td>2.781033</td>\n",
       "      <td>5.467836</td>\n",
       "      <td>2.055202</td>\n",
       "      <td>3.762518</td>\n",
       "      <td>2.999243</td>\n",
       "      <td>3.145068</td>\n",
       "      <td>1.021765</td>\n",
       "      <td>1.380819</td>\n",
       "      <td>5.890095</td>\n",
       "      <td>1.743795</td>\n",
       "      <td>4.014479</td>\n",
       "      <td>3.402704</td>\n",
       "      <td>1.952230</td>\n",
       "      <td>2.918702</td>\n",
       "      <td>2.303800</td>\n",
       "      <td>2.676783</td>\n",
       "      <td>3.119419</td>\n",
       "      <td>5.389834</td>\n",
       "      <td>3.895959</td>\n",
       "      <td>5.529860</td>\n",
       "      <td>1.909896</td>\n",
       "      <td>2.741665</td>\n",
       "      <td>2.222348</td>\n",
       "      <td>4.007038</td>\n",
       "      <td>3.358720</td>\n",
       "      <td>1.541842</td>\n",
       "      <td>5.157278</td>\n",
       "      <td>6.714577</td>\n",
       "      <td>0.701417</td>\n",
       "      <td>1.279493</td>\n",
       "      <td>2.652293</td>\n",
       "      <td>4.975680</td>\n",
       "      <td>5.041458</td>\n",
       "      <td>1.869686</td>\n",
       "      <td>1.622834</td>\n",
       "      <td>3.781793</td>\n",
       "      <td>1.655608</td>\n",
       "      <td>3.906224</td>\n",
       "      <td>1.601470</td>\n",
       "      <td>1.690426</td>\n",
       "      <td>4.331448</td>\n",
       "      <td>5.293569</td>\n",
       "      <td>2.724885</td>\n",
       "      <td>2.525229</td>\n",
       "      <td>1.623036</td>\n",
       "      <td>4.600068</td>\n",
       "      <td>2.769329</td>\n",
       "      <td>2.696338</td>\n",
       "      <td>2.649653</td>\n",
       "      <td>2.229302</td>\n",
       "      <td>2.230606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>18.024460</td>\n",
       "      <td>22.601524</td>\n",
       "      <td>27.918371</td>\n",
       "      <td>28.655079</td>\n",
       "      <td>23.750246</td>\n",
       "      <td>26.077110</td>\n",
       "      <td>16.938271</td>\n",
       "      <td>52.428352</td>\n",
       "      <td>17.541622</td>\n",
       "      <td>18.071888</td>\n",
       "      <td>37.471142</td>\n",
       "      <td>34.626419</td>\n",
       "      <td>24.344589</td>\n",
       "      <td>19.606474</td>\n",
       "      <td>19.500950</td>\n",
       "      <td>33.188717</td>\n",
       "      <td>37.896343</td>\n",
       "      <td>39.378742</td>\n",
       "      <td>55.689831</td>\n",
       "      <td>19.999136</td>\n",
       "      <td>18.624996</td>\n",
       "      <td>64.264793</td>\n",
       "      <td>31.533251</td>\n",
       "      <td>34.177280</td>\n",
       "      <td>36.544731</td>\n",
       "      <td>33.914032</td>\n",
       "      <td>39.658615</td>\n",
       "      <td>18.896595</td>\n",
       "      <td>35.787132</td>\n",
       "      <td>23.288439</td>\n",
       "      <td>27.969589</td>\n",
       "      <td>25.482500</td>\n",
       "      <td>26.554855</td>\n",
       "      <td>29.295904</td>\n",
       "      <td>16.079582</td>\n",
       "      <td>38.024303</td>\n",
       "      <td>49.020081</td>\n",
       "      <td>16.002151</td>\n",
       "      <td>31.777962</td>\n",
       "      <td>15.599897</td>\n",
       "      <td>16.129927</td>\n",
       "      <td>25.712074</td>\n",
       "      <td>24.844378</td>\n",
       "      <td>22.816452</td>\n",
       "      <td>28.650955</td>\n",
       "      <td>14.522635</td>\n",
       "      <td>19.833723</td>\n",
       "      <td>26.017067</td>\n",
       "      <td>26.406811</td>\n",
       "      <td>52.894966</td>\n",
       "      <td>20.034935</td>\n",
       "      <td>25.888702</td>\n",
       "      <td>21.011089</td>\n",
       "      <td>16.331638</td>\n",
       "      <td>28.040071</td>\n",
       "      <td>36.123993</td>\n",
       "      <td>31.370787</td>\n",
       "      <td>43.852356</td>\n",
       "      <td>37.088123</td>\n",
       "      <td>51.386280</td>\n",
       "      <td>24.444849</td>\n",
       "      <td>12.860329</td>\n",
       "      <td>27.972198</td>\n",
       "      <td>24.344355</td>\n",
       "      <td>44.405643</td>\n",
       "      <td>29.702198</td>\n",
       "      <td>19.409576</td>\n",
       "      <td>30.127403</td>\n",
       "      <td>43.549400</td>\n",
       "      <td>39.111294</td>\n",
       "      <td>25.768150</td>\n",
       "      <td>13.165955</td>\n",
       "      <td>12.171367</td>\n",
       "      <td>25.081585</td>\n",
       "      <td>36.036610</td>\n",
       "      <td>45.021458</td>\n",
       "      <td>30.732347</td>\n",
       "      <td>21.328964</td>\n",
       "      <td>14.233596</td>\n",
       "      <td>20.204519</td>\n",
       "      <td>25.459734</td>\n",
       "      <td>33.055000</td>\n",
       "      <td>25.477468</td>\n",
       "      <td>16.207867</td>\n",
       "      <td>58.689289</td>\n",
       "      <td>33.643768</td>\n",
       "      <td>39.746090</td>\n",
       "      <td>23.192484</td>\n",
       "      <td>42.448872</td>\n",
       "      <td>18.263023</td>\n",
       "      <td>22.673632</td>\n",
       "      <td>26.464434</td>\n",
       "      <td>10.437089</td>\n",
       "      <td>24.937464</td>\n",
       "      <td>16.697250</td>\n",
       "      <td>30.774929</td>\n",
       "      <td>28.307819</td>\n",
       "      <td>33.689285</td>\n",
       "      <td>38.154816</td>\n",
       "      <td>41.863590</td>\n",
       "      <td>23.254513</td>\n",
       "      <td>54.681400</td>\n",
       "      <td>22.906368</td>\n",
       "      <td>17.541052</td>\n",
       "      <td>19.323534</td>\n",
       "      <td>26.691010</td>\n",
       "      <td>29.991415</td>\n",
       "      <td>20.578934</td>\n",
       "      <td>30.252144</td>\n",
       "      <td>28.147213</td>\n",
       "      <td>29.936543</td>\n",
       "      <td>17.901352</td>\n",
       "      <td>15.673507</td>\n",
       "      <td>18.643692</td>\n",
       "      <td>22.426868</td>\n",
       "      <td>32.037056</td>\n",
       "      <td>15.735491</td>\n",
       "      <td>16.876789</td>\n",
       "      <td>27.618805</td>\n",
       "      <td>41.005379</td>\n",
       "      <td>39.843224</td>\n",
       "      <td>27.570932</td>\n",
       "      <td>36.733280</td>\n",
       "      <td>14.316000</td>\n",
       "      <td>27.130157</td>\n",
       "      <td>24.707687</td>\n",
       "      <td>13.929532</td>\n",
       "      <td>41.308521</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               DAE_0          DAE_1          DAE_2          DAE_3  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.024674       1.408132       2.937426       2.922235   \n",
       "std         1.017644       0.951307       1.306224       1.491263   \n",
       "min         0.062018      -0.027860       0.812468      -0.017151   \n",
       "25%         1.274684       0.785676       1.880312       1.679957   \n",
       "50%         1.903999       1.213563       2.695305       2.857519   \n",
       "75%         2.549024       1.737550       3.884579       4.069175   \n",
       "max        18.024460      22.601524      27.918371      28.655079   \n",
       "\n",
       "               DAE_4          DAE_5          DAE_6          DAE_7  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        1.404105       2.793133       2.509424       0.836562   \n",
       "std         0.769466       1.364367       1.119864       0.786350   \n",
       "min        -0.026841       0.413867      -0.107802      -0.043650   \n",
       "25%         0.818609       1.780044       1.592092       0.425845   \n",
       "50%         1.307921       2.602454       2.382053       0.670372   \n",
       "75%         1.938737       3.447420       3.468974       1.018479   \n",
       "max        23.750246      26.077110      16.938271      52.428352   \n",
       "\n",
       "               DAE_8          DAE_9         DAE_10         DAE_11  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        3.257326       3.626128       1.956815       1.648261   \n",
       "std         1.459839       1.641880       1.332849       0.882851   \n",
       "min         0.407278       0.543840       0.428674       0.009789   \n",
       "25%         2.094150       2.173276       1.199851       0.996286   \n",
       "50%         3.009487       3.387092       1.621315       1.562461   \n",
       "75%         4.554482       5.058271       2.203326       2.154535   \n",
       "max        17.541622      18.071888      37.471142      34.626419   \n",
       "\n",
       "              DAE_12         DAE_13         DAE_14         DAE_15  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.811153       3.208830       1.484894       2.376671   \n",
       "std         1.600111       1.297801       0.693050       0.802069   \n",
       "min         0.005310       0.384764       0.260447       0.561378   \n",
       "25%         1.575969       2.263134       1.004348       1.771035   \n",
       "50%         2.672050       3.129374       1.344065       2.274192   \n",
       "75%         3.995942       4.280066       1.809230       2.897684   \n",
       "max        24.344589      19.606474      19.500950      33.188717   \n",
       "\n",
       "              DAE_16         DAE_17         DAE_18         DAE_19  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        3.860445       1.679050       1.981682       1.661340   \n",
       "std         1.967745       0.899305       1.037621       0.839485   \n",
       "min         0.375714       0.428371       0.042687      -0.126296   \n",
       "25%         2.307271       1.047377       1.173922       1.087964   \n",
       "50%         3.604648       1.538631       1.930358       1.532042   \n",
       "75%         5.609864       2.103645       2.668835       2.080106   \n",
       "max        37.896343      39.378742      55.689831      19.999136   \n",
       "\n",
       "              DAE_20         DAE_21         DAE_22         DAE_23  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.739988       1.137162       2.801373       2.568943   \n",
       "std         1.533602       1.220607       1.484159       1.360320   \n",
       "min        -0.026918      -0.087905       0.208290       0.238699   \n",
       "25%         1.556829       0.332557       1.569570       1.570461   \n",
       "50%         2.713564       0.840364       2.743024       2.386667   \n",
       "75%         3.823456       1.669661       3.917695       3.408428   \n",
       "max        18.624996      64.264793      31.533251      34.177280   \n",
       "\n",
       "              DAE_24         DAE_25         DAE_26         DAE_27  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        3.491256       4.054435       1.839545       2.412157   \n",
       "std         1.689755       1.931580       0.983720       1.196447   \n",
       "min         0.296108       0.555157       0.144452       0.216937   \n",
       "25%         2.089021       2.467633       1.042034       1.388354   \n",
       "50%         3.336596       4.010700       1.742618       2.298470   \n",
       "75%         4.821830       5.719940       2.491667       3.422202   \n",
       "max        36.544731      33.914032      39.658615      18.896595   \n",
       "\n",
       "              DAE_28         DAE_29         DAE_30         DAE_31  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.417036       0.990277       0.898654       1.914059   \n",
       "std         1.160527       0.896995       0.690273       0.717938   \n",
       "min        -0.074775      -0.193840      -0.185030       0.496002   \n",
       "25%         1.617658       0.300286       0.371273       1.388170   \n",
       "50%         2.378002       0.876855       0.774970       1.765796   \n",
       "75%         3.212009       1.454160       1.361463       2.366352   \n",
       "max        35.787132      23.288439      27.969589      25.482500   \n",
       "\n",
       "              DAE_32         DAE_33         DAE_34         DAE_35  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.411544       2.159241       2.735686       2.314098   \n",
       "std         1.295802       1.374520       1.280010       1.200422   \n",
       "min         0.011655      -0.202392       0.031893       0.503008   \n",
       "25%         1.390193       1.114027       1.639560       1.523963   \n",
       "50%         2.320869       2.010129       2.597027       2.073592   \n",
       "75%         3.420475       2.952448       3.853345       2.790218   \n",
       "max        26.554855      29.295904      16.079582      38.024303   \n",
       "\n",
       "              DAE_36         DAE_37         DAE_38         DAE_39  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        1.951012       1.348031       3.207363       2.653598   \n",
       "std         0.975409       0.463264       1.424147       0.825631   \n",
       "min         0.424149      -0.133708       0.626102       0.554607   \n",
       "25%         1.311072       1.083040       2.038432       2.012628   \n",
       "50%         1.844840       1.268389       3.099233       2.536582   \n",
       "75%         2.371203       1.510947       4.200063       3.229191   \n",
       "max        49.020081      16.002151      31.777962      15.599897   \n",
       "\n",
       "              DAE_40         DAE_41         DAE_42         DAE_43  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        1.316775       3.112838       1.679902       3.475783   \n",
       "std         0.728766       1.511979       0.897245       1.939461   \n",
       "min        -0.091645       0.257875       0.469841       0.014262   \n",
       "25%         0.785273       1.942211       1.123655       1.885470   \n",
       "50%         1.237081       2.995427       1.488312       3.342380   \n",
       "75%         1.829378       4.152659       1.966339       5.220947   \n",
       "max        16.129927      25.712074      24.844378      22.816452   \n",
       "\n",
       "              DAE_44         DAE_45         DAE_46         DAE_47  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.062476       0.473169       0.911093       2.108604   \n",
       "std         1.334288       0.383691       0.545298       1.108517   \n",
       "min         0.167054      -0.169340      -0.061915       0.316752   \n",
       "25%         1.025364       0.178673       0.529758       1.286473   \n",
       "50%         1.793241       0.366063       0.787764       1.947137   \n",
       "75%         2.836496       0.700508       1.175785       2.720881   \n",
       "max        28.650955      14.522635      19.833723      26.017067   \n",
       "\n",
       "              DAE_48         DAE_49         DAE_50         DAE_51  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        1.854478       2.301869       3.907174       2.412498   \n",
       "std         0.976878       1.215984       2.184227       0.924319   \n",
       "min        -0.034165       0.728749       0.165143       0.533683   \n",
       "25%         1.170368       1.611055       2.079033       1.692532   \n",
       "50%         1.761132       2.045162       3.674017       2.281202   \n",
       "75%         2.417540       2.575374       5.706619       2.985958   \n",
       "max        26.406811      52.894966      20.034935      25.888702   \n",
       "\n",
       "              DAE_52         DAE_53         DAE_54         DAE_55  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.093194       2.144174       2.688028       2.562392   \n",
       "std         0.891804       1.138509       1.355214       1.293900   \n",
       "min         0.268449       0.171979       0.017963       0.181411   \n",
       "25%         1.404396       1.185068       1.598881       1.625813   \n",
       "50%         1.987781       2.055080       2.742975       2.437018   \n",
       "75%         2.711728       2.968689       3.683208       3.278931   \n",
       "max        21.011089      16.331638      28.040071      36.123993   \n",
       "\n",
       "              DAE_56         DAE_57         DAE_58         DAE_59  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        0.542321       2.804337       4.070835       2.631412   \n",
       "std         0.511135       1.157645       1.971765       1.186085   \n",
       "min        -0.198090       0.564466       0.237165       0.425648   \n",
       "25%         0.204318       1.878807       2.343534       1.771640   \n",
       "50%         0.472236       2.701438       3.912577       2.448956   \n",
       "75%         0.752838       3.697036       5.807392       3.338212   \n",
       "max        31.370787      43.852356      37.088123      51.386280   \n",
       "\n",
       "              DAE_60         DAE_61         DAE_62         DAE_63  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        3.579354       0.839137       2.388687       3.143724   \n",
       "std         2.114194       0.642434       1.161383       1.706791   \n",
       "min        -0.063608      -0.127744       0.207885       0.154899   \n",
       "25%         1.717301       0.369828       1.607864       1.654081   \n",
       "50%         3.539217       0.703858       2.210787       3.017775   \n",
       "75%         5.447196       1.166043       3.021713       4.654884   \n",
       "max        24.444849      12.860329      27.972198      24.344355   \n",
       "\n",
       "              DAE_64         DAE_65         DAE_66         DAE_67  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        1.632429       2.864942       5.138174       3.223232   \n",
       "std         1.047712       1.429129       3.182129       1.502650   \n",
       "min         0.330341       0.412181      -0.071040       0.522588   \n",
       "25%         1.023143       1.806286       2.388594       1.924301   \n",
       "50%         1.361162       2.649114       5.012832       3.074674   \n",
       "75%         1.904300       3.828641       8.047491       4.484698   \n",
       "max        44.405643      29.702198      19.409576      30.127403   \n",
       "\n",
       "              DAE_68         DAE_69         DAE_70         DAE_71  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        1.306428       3.053071       0.973733       1.451537   \n",
       "std         0.901202       1.649843       0.700494       0.570764   \n",
       "min         0.139419      -0.105705      -0.165483      -0.135702   \n",
       "25%         0.805316       1.677071       0.491612       1.074723   \n",
       "50%         1.132413       2.928179       0.858776       1.347187   \n",
       "75%         1.536645       4.429292       1.284847       1.782634   \n",
       "max        43.549400      39.111294      25.768150      13.165955   \n",
       "\n",
       "              DAE_72         DAE_73         DAE_74         DAE_75  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        0.411263       2.231162       2.314613       2.731990   \n",
       "std         0.589004       0.948936       1.210597       1.108601   \n",
       "min        -0.238229       0.335615       0.121074       0.733622   \n",
       "25%        -0.027381       1.455519       1.415537       1.881561   \n",
       "50%         0.218637       2.169994       2.185149       2.533827   \n",
       "75%         0.629469       2.975945       3.028923       3.476605   \n",
       "max        12.171367      25.081585      36.036610      45.021458   \n",
       "\n",
       "              DAE_76         DAE_77         DAE_78         DAE_79  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        1.031546       2.230681       3.490454       1.545617   \n",
       "std         1.135030       0.820134       2.186010       0.730072   \n",
       "min        -0.223446       0.040887      -0.110965       0.060668   \n",
       "25%         0.212787       1.573933       1.508174       1.000390   \n",
       "50%         0.692259       2.032061       3.251299       1.535620   \n",
       "75%         1.599723       2.781033       5.467836       2.055202   \n",
       "max        30.732347      21.328964      14.233596      20.204519   \n",
       "\n",
       "              DAE_80         DAE_81         DAE_82         DAE_83  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.844125       2.217495       2.502552       0.874606   \n",
       "std         1.240039       1.463566       1.101996       0.394449   \n",
       "min         0.162604      -0.022150       0.490635      -0.148190   \n",
       "25%         1.886210       1.089769       1.688613       0.636378   \n",
       "50%         2.771674       2.049525       2.401886       0.798529   \n",
       "75%         3.762518       2.999243       3.145068       1.021765   \n",
       "max        25.459734      33.055000      25.477468      16.207867   \n",
       "\n",
       "              DAE_84         DAE_85         DAE_86         DAE_87  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        1.035058       4.537579       1.466348       2.935630   \n",
       "std         0.858471       1.737618       0.901441       1.402081   \n",
       "min        -0.103424       1.658968       0.222788       0.346246   \n",
       "25%         0.478779       3.043893       0.938102       1.788649   \n",
       "50%         0.907009       4.226443       1.280626       2.825978   \n",
       "75%         1.380819       5.890095       1.743795       4.014479   \n",
       "max        58.689289      33.643768      39.746090      23.192484   \n",
       "\n",
       "              DAE_88         DAE_89         DAE_90         DAE_91  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.836405       1.596715       2.341712       1.807350   \n",
       "std         1.082996       0.724913       0.810419       0.697783   \n",
       "min         0.520826       0.294630       0.515861       0.408061   \n",
       "25%         2.082508       1.084822       1.712558       1.264002   \n",
       "50%         2.702868       1.484420       2.249470       1.751781   \n",
       "75%         3.402704       1.952230       2.918702       2.303800   \n",
       "max        42.448872      18.263023      22.673632      26.464434   \n",
       "\n",
       "              DAE_92         DAE_93         DAE_94         DAE_95  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.057317       2.665547       3.989386       3.086629   \n",
       "std         0.996474       0.818546       1.682098       1.141823   \n",
       "min         0.029962       1.165267       0.690166       1.097157   \n",
       "25%         1.286618       2.085054       2.641004       2.131509   \n",
       "50%         2.032765       2.512692       3.829549       2.932856   \n",
       "75%         2.676783       3.119419       5.389834       3.895959   \n",
       "max        10.437089      24.937464      16.697250      30.774929   \n",
       "\n",
       "              DAE_96         DAE_97         DAE_98         DAE_99  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        3.927931       1.440984       2.123539       1.637443   \n",
       "std         1.934143       1.159958       1.182307       0.881222   \n",
       "min         0.406276      -0.223543       0.114398       0.034191   \n",
       "25%         2.363745       0.687904       1.334411       0.922157   \n",
       "50%         3.744698       1.293214       1.976575       1.541849   \n",
       "75%         5.529860       1.909896       2.741665       2.222348   \n",
       "max        28.307819      33.689285      38.154816      41.863590   \n",
       "\n",
       "             DAE_100        DAE_101        DAE_102        DAE_103  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.927938       2.510748       1.107941       3.701484   \n",
       "std         1.357506       1.256620       1.149014       1.779679   \n",
       "min        -0.103880      -0.037165      -0.163817       0.524843   \n",
       "25%         1.899177       1.418818       0.326800       1.998526   \n",
       "50%         2.837799       2.649225       0.717668       3.611367   \n",
       "75%         4.007038       3.358720       1.541842       5.157278   \n",
       "max        23.254513      54.681400      22.906368      17.541052   \n",
       "\n",
       "             DAE_104        DAE_105        DAE_106        DAE_107  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        4.635833       0.525721       1.134774       2.043727   \n",
       "std         2.497866       0.577746       0.657375       0.918154   \n",
       "min         0.522679      -0.193335       0.046562       0.129730   \n",
       "25%         2.311509       0.162656       0.761880       1.360384   \n",
       "50%         4.505921       0.361123       0.980042       2.021122   \n",
       "75%         6.714577       0.701417       1.279493       2.652293   \n",
       "max        19.323534      26.691010      29.991415      20.578934   \n",
       "\n",
       "             DAE_108        DAE_109        DAE_110        DAE_111  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        3.750197       3.535721       1.566113       1.331328   \n",
       "std         1.617673       1.759838       0.587936       0.739148   \n",
       "min         0.644681       0.512510       0.019789       0.183774   \n",
       "25%         2.209044       1.993971       1.214948       0.835670   \n",
       "50%         3.601254       3.388418       1.532064       1.182330   \n",
       "75%         4.975680       5.041458       1.869686       1.622834   \n",
       "max        30.252144      28.147213      29.936543      17.901352   \n",
       "\n",
       "             DAE_112        DAE_113        DAE_114        DAE_115  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        2.827906       1.259785       3.121304       1.359910   \n",
       "std         1.285038       0.988571       1.126258       0.787080   \n",
       "min         0.383844      -0.138195       0.060404       0.166807   \n",
       "25%         1.837018       0.490853       2.302200       0.884167   \n",
       "50%         2.772798       1.066043       3.104760       1.203931   \n",
       "75%         3.781793       1.655608       3.906224       1.601470   \n",
       "max        15.673507      18.643692      22.426868      32.037056   \n",
       "\n",
       "             DAE_116        DAE_117        DAE_118        DAE_119  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        1.340247       3.293372       4.014331       2.090719   \n",
       "std         0.931314       1.330197       1.756180       1.174962   \n",
       "min        -0.034977       0.842711       0.526916      -0.187488   \n",
       "25%         0.751478       2.205729       2.557132       1.258192   \n",
       "50%         1.214430       3.117557       3.994386       1.973584   \n",
       "75%         1.690426       4.331448       5.293569       2.724885   \n",
       "max        15.735491      16.876789      27.618805      41.005379   \n",
       "\n",
       "             DAE_120        DAE_121        DAE_122        DAE_123  \\\n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000   \n",
       "mean        1.842517       1.283807       3.295710       2.128874   \n",
       "std         0.873349       1.243757       1.718334       0.978994   \n",
       "min         0.219643      -0.024299       0.127327       0.236253   \n",
       "25%         1.128842       0.519243       1.866449       1.352503   \n",
       "50%         1.644768       0.856074       3.288159       2.051226   \n",
       "75%         2.525229       1.623036       4.600068       2.769329   \n",
       "max        39.843224      27.570932      36.733280      14.316000   \n",
       "\n",
       "             DAE_124        DAE_125        DAE_126        DAE_127  \n",
       "count  428932.000000  428932.000000  428932.000000  428932.000000  \n",
       "mean        2.004336       2.287117       1.753305       1.867003  \n",
       "std         0.886139       0.729555       0.748291       0.698339  \n",
       "min         0.017489       0.418045       0.155278       0.471288  \n",
       "25%         1.392505       1.768940       1.197701       1.392847  \n",
       "50%         2.061236       2.236703       1.635326       1.822362  \n",
       "75%         2.696338       2.649653       2.229302       2.230606  \n",
       "max        27.130157      24.707687      13.929532      41.308521  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output = pd.DataFrame(output.detach().numpy())\n",
    "df_output.columns = ['DAE_'+ str(i) for i in df_output.columns]\n",
    "df_output.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_train, df_output], axis=1)\n",
    "pickle.dump(df, open(os.path.join(fm_path, \"train.pkl\"), 'wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "283baaf4ce8ec1279b6a1eb52777837c25a71c1c72f4c54d87f2b7711be4d242"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('kaggle': pyenv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}